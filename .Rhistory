z=rnorm(1000)
hist(z)
install.packages('glmnet')
rm(list = ls())
#based on https://web.stanford.edu/~hastie/glmnet/glmnet_alpha.html#poi
library('glmnet')
load("PoissonExample.RData")
fit = glmnet(x, y, family = "poisson")
library('devtools')
install.packages('devtools')
devtools::install_github('drvalle1/myfirstpackage1')
devtools::install_github('drvalle1/myfirstpackage')
devtools::install_github('drvalle1/myfirstpackage')
devtools::install_github('drvalle1/myfirstpackage1')
devtools::install_github('drvalle1/myfirstpackage1')
install.packages('devtools')
devtools::install_github('drvalle1/myfirstpackage1')
library('myfirstpackage')
?tnorm
mean1=3; sd1=2; lo1=0; hi1=Inf
z=tnorm(n=10000,lo=lo1,hi=hi1,mu=mean1,sig=sd1)
tmp=density(z,from=lo1)
plot(tmp$x,tmp$y,type='l')
x=seq(from=lo1,to=max(z),length.out=1000)
y=dnorm(x,mean=mean1,sd=sd1)/(1-pnorm(lo1,mean=mean1,sd=sd1))
lines(x,y,col='red')
devtools::install_github('rstudio/rmarkdown')
install.packages('rsconnect')
library('Ecocluster')
library('EcoCluster')
browseVignettes("EcoCluster")
browseVignettes("EcoCluster")
install.packages('tidyverse')
library('devtools')
library('devtools')
install_github('drvalle1/EcoCluster')
browseVignettes("EcoCluster")
?install_github
install_github('drvalle1/EcoCluster',build_vignettes=T)
install_github('drvalle1/EcoCluster',build_vignettes=T,force=T)
browseVignettes("EcoCluster")
install.packages('devtools')
devtools::install_github('tinytex')
install.packages('tinytex')
tinytex:::is_tinytex()
library('prettydoc')
install.packages('prettydoc')
library('prettydoc')
rm(list=ls(all=TRUE))
library(MCMCpack)
set.seed(5)
nloc=5000
nspp=40
ncommun=5
#design matrix
xmat=matrix(rnorm(nloc*ncommun),nloc,ncommun)
#parameters
lambda.true=lambda=runif(ncommun,min=10,max=12)
betas.true=betas=diag(1,ncommun)
#get means
lambda1=matrix(lambda,nloc,ncommun,byrow=T)
media=exp(log(lambda1)+xmat%*%betas); range(media)
#generate N_lk
nlk=matrix(NA,nloc,ncommun)
for (i in 1:ncommun){
nlk[,i]=rpois(nloc,media[,i])
}
nlk.true=nlk; boxplot(nlk)
z=nlk/apply(nlk,1,sum); apply(z,1,sum); boxplot(z); apply(z,2,range)
nl=apply(nlk,1,sum)
hist(nl)
sum(nl)
#generate phi (assuming that each species is strongly present in a single group)
phi=rdirichlet(ncommun,alpha=rep(0.1,nspp))
# for (i in 1:nspp){ #add some zeroes
#   ind=sample(1:ncommun,size=1)
#   tmp[ind,i]=runif(1,min=0.5,max=1)
# }
# phi=tmp/matrix(rowSums(tmp),ncommun,nspp) #re-scale to make sure it sums to 1
# round(phi[,1:20],2)
# table(round(phi,2))
unique(rowSums(phi))
phi.true=phi
image(phi)
#generate actual observations y
y=matrix(NA,nloc,nspp)
nks=matrix(0,ncommun,nspp)
for (i in 1:nloc){
tmp1=rep(0,nspp)
for (k in 1:ncommun){
tmp=rmultinom(1,size=nlk[i,k],prob=phi[k,])
nks[k,]=nks[k,]+tmp
tmp1=tmp1+tmp
}
y[i,]=tmp1
}
image(y)
#look at stuff to make sure it makes sense
phi.estim=nks/matrix(rowSums(nks),ncommun,nspp,)
plot(phi.true,phi.estim)
nks.true=nks
#export results
setwd('U:\\GIT_models\\git_LDA_MS')
nome=paste('fake data',ncommun,'.csv',sep='')
colnames(y)=paste('spp',1:nspp,sep='')
rownames(y)=paste('loc',1:nloc,sep='')
write.csv(y,nome,row.names=F)
nome=paste('fake data xmat',ncommun,'.csv',sep='')
write.csv(xmat,nome,row.names=F)
library('Rcpp')
library('RcppArmadillo')
library(inline)
set.seed(4)
#get functions
setwd('U:\\GIT_models\\git_LDA_MS')
source('gibbs functions.R')
sourceCpp('aux1.cpp')
#get data
dat=read.csv('fake data5.csv',as.is=T)
xmat=data.matrix(read.csv('fake data xmat5.csv',as.is=T))
y=data.matrix(dat)
#basic settings
ncomm=5
ngibbs=1000
nburn=ngibbs/2
nparam=ncol(xmat)
nloc=nrow(dat)
nspp=ncol(dat)
#priors
phi.prior=0.1
lambda.a=lambda.b=0.1
betas=matrix(0,nparam,ncomm)
#initial values
array.lsk=array(0,dim=c(nloc,nspp,ncomm))
for (i in 1:nloc){
for (j in 1:nspp){
if (y[i,j]!=0){
array.lsk[i,j,]=rmultinom(1,size=y[i,j],prob=rep(1/ncomm,ncomm))
}
}
}
#basic test
# z=apply(array.lsk,1:2,sum)
# unique(y-z)
nlk=apply(array.lsk,c(1,3),sum)
nks=t(apply(array.lsk,2:3,sum))
lambda=apply(nlk,2,mean)
phi=matrix(1/nspp,ncomm,nspp)
#to store outcomes from gibbs sampler
lambda.out=matrix(NA,ngibbs,ncomm)
phi.out=matrix(NA,ngibbs,nspp*ncomm)
nlk.out=matrix(NA,ngibbs,nloc*ncomm)
llk.out=rep(NA,ngibbs)
betas.out=matrix(NA,ngibbs,nparam*ncomm)
#useful stuff for MH algorithm
accept1=list(betas=matrix(0,nparam,ncomm))
jump1=list(betas=matrix(1,nparam,ncomm))
accept.output=50
nadapt=ngibbs/2
#run gibbs sampler
options(warn=2)
for (i in 1:ngibbs){
print(i)
#get log mean
llambda=matrix(log(lambda),nloc,ncomm,byrow=T)
lmedia=llambda+xmat%*%betas
#sample z
tmp=samplez.R(lphi=log(phi), lmedia=lmedia, array.lsk=array.lsk, y=y,
nlk=nlk, ncomm=ncomm,nloc=nloc, nspp=nspp)
nlk=tmp$nlk
array.lsk=tmp$array.lsk
nks=t(apply(array.lsk,2:3,sum))
#sample phi
# phi=rdirichlet1(alpha=nks+phi.prior,ncomm=ncomm,nspp=nspp)
phi=phi.true
#sample betas
tmp=sample.betas(lambda.a=lambda.a,lambda.b=lambda.b,nlk=nlk,xmat=xmat,betas=betas,
ncomm=ncomm,nparam=nparam,jump1=jump1$betas)
betas=tmp$betas
accept1$betas=accept1$betas+tmp$accept
# betas=betas.true
#sample lambdas
lambda=sample.lambdas(lambda.a=lambda.a,lambda.b=lambda.b,nlk=nlk,ncomm=ncomm,nloc=nloc,
xmat=xmat,betas=betas)
# lambda=lambda.true
#adaptive MH
if (i%%accept.output==0 & i<nadapt){
k=print.adapt(accept1z=accept1,jump1z=jump1,accept.output=accept.output)
accept1=k$accept1
jump1=k$jump1
}
#calculate loglikelihood
p1=dpois(nlk,matrix(lambda,nloc,ncomm,byrow=T),log=T)
phi.tmp=phi; phi.tmp[phi.tmp<0.00001]=0.00001
p2=nks*log(phi.tmp)
#store results
llk.out[i]=sum(p1)+sum(p2)
phi.out[i,]=phi
lambda.out[i,]=lambda
nlk.out[i,]=nlk
betas.out[i,]=betas
}
plot(llk.out[100:ngibbs],type='l')
compare1=function(estim,true){
rango=range(c(true,estim))
plot(true,estim,ylim=rango,xlim=rango)
lines(rango,rango)
}
#we need to re-order things first
compare1(estim=lambda.out[ngibbs,],true=lambda.true)
compare1(estim=nlk.out[ngibbs,],true=nlk.true)
betas
compare1(estim=betas.out[ngibbs,],true=betas.true)
compare1(estim=nlk.out[ngibbs,],true=nlk.true)
compare1(estim=phi.out[ngibbs,],true=phi.true)
compare1(estim=nlk.out[ngibbs,],true=nlk.true)
compare1(estim=lambda.out[ngibbs,],true=lambda.true)
apply(xmat,2,mean)
library('Rcpp')
library('RcppArmadillo')
set.seed(4)
#get functions
setwd('U:\\GIT_models\\git_LDA_MS')
source('gibbs functions.R')
sourceCpp('aux1.cpp')
#get data
dat=read.csv('fake data5.csv',as.is=T)
xmat=data.matrix(read.csv('fake data xmat5.csv',as.is=T))
y=data.matrix(dat)
#basic settings
ncomm=5
ngibbs=1000
nburn=ngibbs/2
nparam=ncol(xmat)
nloc=nrow(dat)
nspp=ncol(dat)
#priors
phi.prior=0.1
lambda.a=lambda.b=0.1
betas=matrix(0,nparam,ncomm)
#initial values
array.lsk=array(0,dim=c(nloc,nspp,ncomm))
for (i in 1:nloc){
for (j in 1:nspp){
if (y[i,j]!=0){
array.lsk[i,j,]=rmultinom(1,size=y[i,j],prob=rep(1/ncomm,ncomm))
}
}
}
#basic test
# z=apply(array.lsk,1:2,sum)
# unique(y-z)
nlk=apply(array.lsk,c(1,3),sum)
nks=t(apply(array.lsk,2:3,sum))
lambda=apply(nlk,2,mean)
phi=matrix(1/nspp,ncomm,nspp)
#to store outcomes from gibbs sampler
lambda.out=matrix(NA,ngibbs,ncomm)
phi.out=matrix(NA,ngibbs,nspp*ncomm)
nlk.out=matrix(NA,ngibbs,nloc*ncomm)
llk.out=rep(NA,ngibbs)
betas.out=matrix(NA,ngibbs,nparam*ncomm)
#useful stuff for MH algorithm
accept1=list(betas=matrix(0,nparam,ncomm))
jump1=list(betas=matrix(1,nparam,ncomm))
accept.output=50
nadapt=ngibbs/2
#run gibbs sampler
options(warn=2)
for (i in 1:ngibbs){
print(i)
#get log mean
llambda=matrix(log(lambda),nloc,ncomm,byrow=T)
lmedia=llambda+xmat%*%betas
#sample z
tmp=samplez.R(lphi=log(phi), lmedia=lmedia, array.lsk=array.lsk, y=y,
nlk=nlk, ncomm=ncomm,nloc=nloc, nspp=nspp)
nlk=tmp$nlk
array.lsk=tmp$array.lsk
nks=t(apply(array.lsk,2:3,sum))
#sample phi
phi=rdirichlet1(alpha=nks+phi.prior,ncomm=ncomm,nspp=nspp)
# phi=phi.true
#sample betas
tmp=sample.betas(lambda.a=lambda.a,lambda.b=lambda.b,nlk=nlk,xmat=xmat,betas=betas,
ncomm=ncomm,nparam=nparam,jump1=jump1$betas)
betas=tmp$betas
accept1$betas=accept1$betas+tmp$accept
# betas=betas.true
#sample lambdas
# lambda=sample.lambdas(lambda.a=lambda.a,lambda.b=lambda.b,nlk=nlk,ncomm=ncomm,nloc=nloc,
#                       xmat=xmat,betas=betas)
lambda=lambda.true
#adaptive MH
if (i%%accept.output==0 & i<nadapt){
k=print.adapt(accept1z=accept1,jump1z=jump1,accept.output=accept.output)
accept1=k$accept1
jump1=k$jump1
}
#calculate loglikelihood
p1=dpois(nlk,matrix(lambda,nloc,ncomm,byrow=T),log=T)
phi.tmp=phi; phi.tmp[phi.tmp<0.00001]=0.00001
p2=nks*log(phi.tmp)
#store results
llk.out[i]=sum(p1)+sum(p2)
phi.out[i,]=phi
lambda.out[i,]=lambda
nlk.out[i,]=nlk
betas.out[i,]=betas
}
plot(llk.out[100:ngibbs],type='l')
compare1=function(estim,true){
rango=range(c(true,estim))
plot(true,estim,ylim=rango,xlim=rango)
lines(rango,rango)
}
#we need to re-order things first
compare1(estim=lambda.out[ngibbs,],true=lambda.true)
compare1(estim=nlk.out[ngibbs,],true=nlk.true)
compare1(estim=betas.out[ngibbs,],true=betas.true)
library('Rcpp')
library('RcppArmadillo')
set.seed(4)
#get functions
setwd('U:\\GIT_models\\git_LDA_MS')
source('gibbs functions.R')
sourceCpp('aux1.cpp')
#get data
dat=read.csv('fake data5.csv',as.is=T)
xmat=data.matrix(read.csv('fake data xmat5.csv',as.is=T))
y=data.matrix(dat)
#basic settings
ncomm=5
ngibbs=1000
nburn=ngibbs/2
nparam=ncol(xmat)
nloc=nrow(dat)
nspp=ncol(dat)
#priors
phi.prior=0.1
lambda.a=lambda.b=0.1
betas=matrix(0,nparam,ncomm)
#initial values
array.lsk=array(0,dim=c(nloc,nspp,ncomm))
for (i in 1:nloc){
for (j in 1:nspp){
if (y[i,j]!=0){
array.lsk[i,j,]=rmultinom(1,size=y[i,j],prob=rep(1/ncomm,ncomm))
}
}
}
#basic test
# z=apply(array.lsk,1:2,sum)
# unique(y-z)
nlk=apply(array.lsk,c(1,3),sum)
nks=t(apply(array.lsk,2:3,sum))
lambda=apply(nlk,2,mean)
phi=matrix(1/nspp,ncomm,nspp)
#to store outcomes from gibbs sampler
lambda.out=matrix(NA,ngibbs,ncomm)
phi.out=matrix(NA,ngibbs,nspp*ncomm)
nlk.out=matrix(NA,ngibbs,nloc*ncomm)
llk.out=rep(NA,ngibbs)
betas.out=matrix(NA,ngibbs,nparam*ncomm)
#useful stuff for MH algorithm
accept1=list(betas=matrix(0,nparam,ncomm))
jump1=list(betas=matrix(1,nparam,ncomm))
accept.output=50
nadapt=ngibbs/2
#run gibbs sampler
options(warn=2)
for (i in 1:ngibbs){
print(i)
#get log mean
llambda=matrix(log(lambda),nloc,ncomm,byrow=T)
lmedia=llambda+xmat%*%betas
#sample z
tmp=samplez.R(lphi=log(phi), lmedia=lmedia, array.lsk=array.lsk, y=y,
nlk=nlk, ncomm=ncomm,nloc=nloc, nspp=nspp)
nlk=tmp$nlk
array.lsk=tmp$array.lsk
nks=t(apply(array.lsk,2:3,sum))
#sample phi
# phi=rdirichlet1(alpha=nks+phi.prior,ncomm=ncomm,nspp=nspp)
phi=phi.true
#sample betas
tmp=sample.betas(lambda.a=lambda.a,lambda.b=lambda.b,nlk=nlk,xmat=xmat,betas=betas,
ncomm=ncomm,nparam=nparam,jump1=jump1$betas)
betas=tmp$betas
accept1$betas=accept1$betas+tmp$accept
# betas=betas.true
#sample lambdas
lambda=sample.lambdas(lambda.a=lambda.a,lambda.b=lambda.b,nlk=nlk,ncomm=ncomm,nloc=nloc,
xmat=xmat,betas=betas)
# lambda=lambda.true
#adaptive MH
if (i%%accept.output==0 & i<nadapt){
k=print.adapt(accept1z=accept1,jump1z=jump1,accept.output=accept.output)
accept1=k$accept1
jump1=k$jump1
}
#calculate loglikelihood
p1=dpois(nlk,matrix(lambda,nloc,ncomm,byrow=T),log=T)
phi.tmp=phi; phi.tmp[phi.tmp<0.00001]=0.00001
p2=nks*log(phi.tmp)
#store results
llk.out[i]=sum(p1)+sum(p2)
phi.out[i,]=phi
lambda.out[i,]=lambda
nlk.out[i,]=nlk
betas.out[i,]=betas
}
plot(llk.out[100:ngibbs],type='l')
compare1=function(estim,true){
rango=range(c(true,estim))
plot(true,estim,ylim=rango,xlim=rango)
lines(rango,rango)
}
#we need to re-order things first
compare1(estim=lambda.out[ngibbs,],true=lambda.true)
compare1(estim=nlk.out[ngibbs,],true=nlk.true)
compare1(estim=betas.out[ngibbs,],true=betas.true)
betas
plot(betas.out[,1],type='l')
par(mfrow=c(4,3),mar=rep(1,4))
for (i in 1:12){
plot(betas.out[,i],type='l')
}
jump
jump1
par(mfrow=c(3,2),mar=rep(1,4))
for (i in 1:ncomm){
plot(lambda.out[,i],type='l')
}
compare1(estim=phi.out[ngibbs,],true=phi.true)
compare1(estim=lambda.out[ngibbs,],true=lambda.true)
