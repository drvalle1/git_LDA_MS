nparam=ncol(xmat)
k1=matrix(k,nparam,ncomm); round(k1,2)
ordem=rep(NA,ncomm)
for (i in 2:nparam){
ordem[i-1]=which(k1[i,]==max(k1[i,]))
}
round(k1[,ordem],2)
plot(res$betas[nburn:ngibbs,1],type='l')
plot(res$NBN,type='l')
compare1=function(estim,true){
rango=range(c(true,estim))
plot(true,estim,ylim=rango,xlim=rango)
lines(rango,rango,col='red',lwd=2)
}
k=res$betas[ngibbs,]
nparam=ncol(xmat)
k1=matrix(k,nparam,ncomm); round(k1,2)
ncomm
ordem=rep(NA,ncomm)
for (i in 2:nparam){
ordem[i-1]=which(k1[i,]==max(k1[i,]))
}
round(k1[,ordem],2)
k1=matrix(k,nparam,ncomm); round(k1,2)
dim(res$betas)
nparam
ordem=rep(NA,ncomm)
for (i in 2:nparam){
ordem[i-1]=which(k1[i,]==max(k1[i,]))
}
round(k1[,ordem],2)
image(k1[,ordem])
#look at nlk
par(mfrow=c(1,1))
tmp=matrix(res$nlk[ngibbs,],nloc,ncomm);
boxplot(tmp)
compare1(estim=jitter(tmp[,ordem]),true=jitter(nlk.true))
rm(list=ls(all=TRUE))
library(MCMCpack)
library('Rcpp')
set.seed(10)
#get data
setwd('U:\\GIT_models\\git_LDA_MS')
dat=read.csv('fake data.csv',as.is=T)
xmat=data.matrix(read.csv('fake data xmat.csv',as.is=T))
# xmat=xmat[,1:2]
y=data.matrix(dat)
nloc=nrow(y)
nspp=ncol(y)
#get array.lsk
tmp=read.csv('array lsk.csv',as.is=T)
ncomm.init=length(tmp$V1)/(nspp*nloc); ncomm.init
array.lsk.init=array(tmp$V1,dim=c(nloc,nspp,ncomm.init))
#get phi
phi.init=data.matrix(read.csv('phi step1.csv',as.is=T))
#basic settings
ngibbs=1000
nburn=ngibbs/2
#priors
phi.prior=0.01
var.betas=c(10,rep(10,ncol(xmat)-1))
gamma=0.1
#----------------------------------------------------------
#determine optimal number of groups
nlk=apply(array.lsk.init,c(1,3),sum)
theta1=nlk/apply(nlk,1,sum)
par(mfrow=c(1,1),mar=c(3,3,1,1))
boxplot(theta1)
#----------------------------------------------------------
#LDA with covariates
#get functions
setwd('U:\\GIT_models\\git_LDA_MS')
source('LDA cov main function.R')
source('LDA cov aux functions.R')
sourceCpp('LDA_cov_aux1_cpp.cpp')
sourceCpp('slice_betas.cpp')
sourceCpp('slice_NBN.cpp')
#re-estimate phi
ncomm=4
nparam=ncol(xmat)
nloc=nrow(y)
nspp=ncol(y)
ntot=apply(y,1,sum)
#get phi by eliminating superfluous groups
ncomm.init=ncol(phi.init)/nspp
ncomm.init
ncomm
tmp=matrix(1:(ncomm.init*nspp),ncomm.init,nspp)
seq1=1:ncomm
ind1=tmp[-seq1,] #indicators for superfluous groups
tmp=colMeans(phi.init[,-ind1])
omd1
ind1
length(tmp)
nparam=ncol(xmat)
nloc=nrow(y)
nspp=ncol(y)
ntot=apply(y,1,sum)
#get phi by eliminating superfluous groups
ncomm.init=ncol(phi.init)/nspp
tmp=matrix(1:(ncomm.init*nspp),ncomm.init,nspp)
seq1=1:ncomm
ind1=tmp[-seq1,] #indicators for superfluous groups
phi.mat=phi.init[,-ind1]
phi.nrow=nrow(phi.mat)
if (!estimate.phi) phi=matrix(phi.mat[1,],ncomm,nspp)
if (estimate.phi)  phi=matrix(phi.mat[phi.nrow,],ncomm,nspp)
nparam=ncol(xmat)
nloc=nrow(y)
nspp=ncol(y)
ntot=apply(y,1,sum)
#get phi by eliminating superfluous groups
ncomm.init=ncol(phi.init)/nspp
tmp=matrix(1:(ncomm.init*nspp),ncomm.init,nspp)
seq1=1:ncomm
ind1=tmp[-seq1,] #indicators for superfluous groups
phi.mat=phi.init[,-ind1]
phi.nrow=nrow(phi.mat)
phi=matrix(phi.mat[1,],ncomm,nspp)
phi=matrix(phi.mat[phi.nrow,],ncomm,nspp)
dim(array.lsk.init)
nlk=apply(array.lsk.init,c(1,3),sum)
theta1=nlk/apply(nlk,1,sum)
boxplot(theta1)
seq1=1:ncomm
array.lsk=array.lsk.init[,,seq1]
for (i in 1:nloc){
for (j in 1:nspp){
tmp=array.lsk.init[i,j,-seq1]
n=sum(tmp)
if (n>0){
prob=theta1[i,seq1]*phi[seq1,j]
prob=prob/sum(prob)
z=rmultinom(1,size=n,prob=prob)
array.lsk[i,j,]=array.lsk[i,j,]+z
}
}
}
#initial values
nlk=apply(array.lsk,c(1,3),sum)
betas=matrix(0,nparam,ncomm)
options(warn=-1) #sometimes I get "glm.fit: fitted rates numerically 0 occurred" here
for (i in 1:ncomm){
dat.tmp=cbind(nlk[,i],xmat[,-1])
colnames(dat.tmp)=rep('',ncol(dat.tmp)) #this is important otherwise next line breaks when we only have a single covariate
colnames(dat.tmp)[1]='y'
dat.tmp1=as.data.frame(dat.tmp)
res=try(glm.nb(y ~ ., data = dat.tmp1),silent=T)
#if we run into an error using NB regression, use Poisson reg
ind=grep('Error',res)
if (length(ind)>0) res=glm(y~.,data=dat.tmp1,family='poisson')
betas[,i]=res$coef
}
options(warn=2)
nks=t(apply(array.lsk,2:3,sum))
nk=rowSums(nks)
# phi=nks/apply(nks,1,sum); apply(phi,1,sum)
NBN=10
betas
#to store outcomes from gibbs sampler
phi.out=matrix(NA,ngibbs,nspp*ncomm)
nlk.out=matrix(NA,ngibbs,nloc*ncomm)
llk.out=rep(NA,ngibbs)
fmodel.out=matrix(NA,ngibbs,1)
betas.out=matrix(NA,ngibbs,nparam*ncomm)
NBN.out=matrix(NA,ngibbs,1)
#useful stuff for slice sampler algorithm
w.betas=1
w.NBN=10
MaxIter=100 #to avoid overly long slice samplers
#to avoid numerical issues when calculating log(p) or log(1-p)
LoThresh=0.00000001
UpThresh=1-LoThresh
#run gibbs sampler
options(warn=2)
media=exp(xmat%*%betas) #get mean
NBN=SampleNBN(Media=media,y=nlk,NBN=NBN,w=w.NBN,MaxIter=MaxIter,LoThresh=LoThresh)
betas=SampleBetas(param=betas,y=nlk,xmat=xmat,w=w.betas,nparam=nparam,
ncomm=ncomm,var1=var.betas,NBN=NBN,MaxIter=MaxIter,
LoThresh=LoThresh)
if (estimate.phi)  phi=rdirichlet1(alpha=nks+phi.prior,ncomm=ncomm,nspp=nspp)
if (!estimate.phi){
oo=sample(phi.nrow,size=1)
phi=matrix(phi.mat[oo,],ncomm,nspp)
}
phi=rdirichlet1(alpha=nks+phi.prior,ncomm=ncomm,nspp=nspp)
oo=sample(phi.nrow,size=1)
phi=matrix(phi.mat[oo,],ncomm,nspp)
rm(list=ls(all=TRUE))
library(MCMCpack)
library('Rcpp')
set.seed(10)
#get data
setwd('U:\\GIT_models\\git_LDA_MS')
dat=read.csv('fake data.csv',as.is=T)
xmat=data.matrix(read.csv('fake data xmat.csv',as.is=T))
# xmat=xmat[,1:2]
y=data.matrix(dat)
nloc=nrow(y)
nspp=ncol(y)
#get array.lsk
tmp=read.csv('array lsk.csv',as.is=T)
ncomm.init=length(tmp$V1)/(nspp*nloc); ncomm.init
array.lsk.init=array(tmp$V1,dim=c(nloc,nspp,ncomm.init))
#get phi
phi.init=data.matrix(read.csv('phi step1.csv',as.is=T))
#basic settings
ngibbs=1000
nburn=ngibbs/2
#priors
phi.prior=0.01
var.betas=c(10,rep(10,ncol(xmat)-1))
gamma=0.1
#----------------------------------------------------------
#determine optimal number of groups
nlk=apply(array.lsk.init,c(1,3),sum)
theta1=nlk/apply(nlk,1,sum)
par(mfrow=c(1,1),mar=c(3,3,1,1))
boxplot(theta1)
#----------------------------------------------------------
#LDA with covariates
#get functions
setwd('U:\\GIT_models\\git_LDA_MS')
source('LDA cov main function.R')
source('LDA cov aux functions.R')
sourceCpp('LDA_cov_aux1_cpp.cpp')
sourceCpp('slice_betas.cpp')
sourceCpp('slice_NBN.cpp')
#re-estimate phi
ncomm=4
res=gibbs.LDA.cov(ncomm=ncomm,ngibbs=ngibbs,nburn=nburn,y=y,xmat=xmat,
phi.prior=phi.prior,array.lsk.init=array.lsk.init,
var.betas=var.betas,estimate.phi=T,
phi.init=phi.init)
library(MCMCpack)
set.seed(201)
nloc=2000
nspp=200
ncommun=4
#design matrix
xmat=matrix(runif(nloc*ncommun,min=-1,max=3),nloc,ncommun)
#pure sites
tmp=matrix(-3,ncommun,ncommun)
diag(tmp)=2
num1=floor(nloc/ncommun)
for (i in 1:300){
seq1=(ncommun*(i-1)+1):(ncommun*i)
xmat[seq1,]=tmp
}
image(xmat)
xmat=cbind(1,xmat)
#parameters
b0=log(runif(ncommun,min=5,max=8))
betas.true=betas=rbind(b0,diag(1,ncommun))
#get means
media.true=media=exp(xmat%*%betas); range(media)
head(media)
#generate N_lk
par(mfrow=c(1,1),mar=rep(4,4))
nlk=matrix(NA,nloc,ncommun)
NBN=20 #when this is large, we get into areas with relatively flat loglikel, giving trouble to the slice sampler
for (i in 1:ncommun){
nlk[,i]=rnbinom(nloc,mu=media[,i],size=NBN)
}
nlk.true=nlk; boxplot(nlk)
soma=apply(nlk,1,sum); sum(soma==0)
theta.true=theta=nlk/soma; apply(theta,1,sum);
boxplot(theta); apply(theta,2,range); apply(theta>0.95,2,mean)
nl=apply(nlk,1,sum)
hist(nl)
sum(nl)
plot(media,nlk)
#generate phi:
#- assume that each species is strongly present in a single group
#- Avoid very rare species (species that are almost never present)
tmp=matrix(0.05,ncommun,nspp)
base=nspp/ncommun
margin1=floor(base*0.2)
seq1=c(seq(from=1,to=nspp,by=base),nspp-margin1)
for (i in 1:(length(seq1)-1)){ #add some zeroes
seq2=seq1[i]:(seq1[i+1]+margin1)
tmp[i,seq2]=1
}
phi=tmp/matrix(rowSums(tmp),ncommun,nspp) #re-scale to make sure it sums to 1
phi.true=phi
# round(phi[,1:20],2)
# table(round(phi,2))
unique(rowSums(phi))
phi.true=phi
image(phi)
#generate actual observations y
array.lsk=array(0,dim=c(nloc,nspp,ncommun))
for (i in 1:nloc){
for (k in 1:ncommun){
array.lsk[i,,k]=rmultinom(1,size=nlk[i,k],prob=phi[k,])
}
}
array.lsk.true=array.lsk
y=apply(array.lsk,c(1,2),sum)
nks=t(apply(array.lsk,c(2,3),sum))
image(y)
phi.estim=nks/rowSums(nks)
rango=range(c(phi,phi.estim))
plot(phi,phi.estim,ylim=rango,xlim=rango)
lines(rango,rango,col='red')
#checking if it makes sense
plot(apply(array.lsk,c(1,3),sum),nlk)
lines(c(0,1000),c(0,1000))
#look at stuff to make sure it makes sense
phi.estim=nks/matrix(rowSums(nks),ncommun,nspp)
plot(phi.true,phi.estim)
nks.true=nks
library(MCMCpack)
library('coda')
plot(res$llk[1:ngibbs],type='l')
nburn=100
plot(res$llk[nburn:ngibbs],type='l')
#calculate effective sample size
effectiveSize(mcmc(res$betas[nburn:ngibbs,]))
plot(res$betas[nburn:ngibbs,1],type='l')
plot(res$NBN,type='l')
compare1=function(estim,true){
rango=range(c(true,estim))
plot(true,estim,ylim=rango,xlim=rango)
lines(rango,rango,col='red',lwd=2)
}
k=res$betas[ngibbs,]
nparam=ncol(xmat)
k1=matrix(k,nparam,ncomm); round(k1,2)
ordem=rep(NA,ncomm)
for (i in 2:nparam){
ordem[i-1]=which(k1[i,]==max(k1[i,]))
}
round(k1[,ordem],2)
ordem=rep(NA,ncomm)
for (i in 2:nparam){
ordem[i-1]=which(k1[i,]==max(k1[i,]))
}
round(k1[,ordem],2)
image(k1[,ordem])
par(mfrow=c(1,1))
tmp=matrix(res$nlk[ngibbs,],nloc,ncomm);
boxplot(tmp)
compare1(estim=jitter(tmp[,ordem]),true=jitter(nlk.true))
#look at betas
k=matrix(res$betas[ngibbs,],nparam,ncomm)
compare1(estim=k[,ordem],true=betas.true)
#look at phi
tmp=matrix(res$phi[ngibbs,],ncomm,nspp)
tmp1=tmp[ordem,]
compare1(estim=tmp1,true=phi.true)
rm(list=ls(all=TRUE))
library(MCMCpack)
library('Rcpp')
set.seed(10)
#get data
setwd('U:\\GIT_models\\git_LDA_MS')
dat=read.csv('fake data.csv',as.is=T)
xmat=data.matrix(read.csv('fake data xmat.csv',as.is=T))
# xmat=xmat[,1:2]
y=data.matrix(dat)
nloc=nrow(y)
nspp=ncol(y)
#get array.lsk
tmp=read.csv('array lsk.csv',as.is=T)
ncomm.init=length(tmp$V1)/(nspp*nloc); ncomm.init
array.lsk.init=array(tmp$V1,dim=c(nloc,nspp,ncomm.init))
#get phi
phi.init=data.matrix(read.csv('phi step1.csv',as.is=T))
#basic settings
ngibbs=1000
nburn=ngibbs/2
#priors
phi.prior=0.01
var.betas=c(10,rep(10,ncol(xmat)-1))
gamma=0.1
#----------------------------------------------------------
#determine optimal number of groups
nlk=apply(array.lsk.init,c(1,3),sum)
theta1=nlk/apply(nlk,1,sum)
par(mfrow=c(1,1),mar=c(3,3,1,1))
boxplot(theta1)
#----------------------------------------------------------
#LDA with covariates
#get functions
setwd('U:\\GIT_models\\git_LDA_MS')
source('LDA cov main function.R')
source('LDA cov aux functions.R')
sourceCpp('LDA_cov_aux1_cpp.cpp')
sourceCpp('slice_betas.cpp')
sourceCpp('slice_NBN.cpp')
#re-estimate phi
ncomm=4
res=gibbs.LDA.cov(ncomm=ncomm,ngibbs=ngibbs,nburn=nburn,y=y,xmat=xmat,
phi.prior=phi.prior,array.lsk.init=array.lsk.init,
var.betas=var.betas,phi.mat=phi.init,estimate.phi=F)
res=gibbs.LDA.cov(ncomm=ncomm,ngibbs=ngibbs,nburn=nburn,y=y,xmat=xmat,
phi.prior=phi.prior,array.lsk.init=array.lsk.init,
var.betas=var.betas,phi.init=phi.init,estimate.phi=F)
library(MCMCpack)
set.seed(201)
nloc=2000
nspp=200
ncommun=4
#design matrix
xmat=matrix(runif(nloc*ncommun,min=-1,max=3),nloc,ncommun)
#pure sites
tmp=matrix(-3,ncommun,ncommun)
diag(tmp)=2
num1=floor(nloc/ncommun)
for (i in 1:300){
seq1=(ncommun*(i-1)+1):(ncommun*i)
xmat[seq1,]=tmp
}
image(xmat)
xmat=cbind(1,xmat)
#parameters
b0=log(runif(ncommun,min=5,max=8))
betas.true=betas=rbind(b0,diag(1,ncommun))
#get means
media.true=media=exp(xmat%*%betas); range(media)
head(media)
#generate N_lk
par(mfrow=c(1,1),mar=rep(4,4))
nlk=matrix(NA,nloc,ncommun)
NBN=20 #when this is large, we get into areas with relatively flat loglikel, giving trouble to the slice sampler
for (i in 1:ncommun){
nlk[,i]=rnbinom(nloc,mu=media[,i],size=NBN)
}
nlk.true=nlk; boxplot(nlk)
soma=apply(nlk,1,sum); sum(soma==0)
theta.true=theta=nlk/soma; apply(theta,1,sum);
boxplot(theta); apply(theta,2,range); apply(theta>0.95,2,mean)
nl=apply(nlk,1,sum)
hist(nl)
sum(nl)
plot(media,nlk)
#generate phi:
#- assume that each species is strongly present in a single group
#- Avoid very rare species (species that are almost never present)
tmp=matrix(0.05,ncommun,nspp)
base=nspp/ncommun
margin1=floor(base*0.2)
seq1=c(seq(from=1,to=nspp,by=base),nspp-margin1)
for (i in 1:(length(seq1)-1)){ #add some zeroes
seq2=seq1[i]:(seq1[i+1]+margin1)
tmp[i,seq2]=1
}
phi=tmp/matrix(rowSums(tmp),ncommun,nspp) #re-scale to make sure it sums to 1
phi.true=phi
# round(phi[,1:20],2)
# table(round(phi,2))
unique(rowSums(phi))
phi.true=phi
image(phi)
#generate actual observations y
array.lsk=array(0,dim=c(nloc,nspp,ncommun))
for (i in 1:nloc){
for (k in 1:ncommun){
array.lsk[i,,k]=rmultinom(1,size=nlk[i,k],prob=phi[k,])
}
}
array.lsk.true=array.lsk
y=apply(array.lsk,c(1,2),sum)
nks=t(apply(array.lsk,c(2,3),sum))
image(y)
phi.estim=nks/rowSums(nks)
rango=range(c(phi,phi.estim))
plot(phi,phi.estim,ylim=rango,xlim=rango)
lines(rango,rango,col='red')
#checking if it makes sense
plot(apply(array.lsk,c(1,3),sum),nlk)
lines(c(0,1000),c(0,1000))
#look at stuff to make sure it makes sense
phi.estim=nks/matrix(rowSums(nks),ncommun,nspp)
plot(phi.true,phi.estim)
nks.true=nks
library(MCMCpack)
library('coda')
plot(res$llk[1:ngibbs],type='l')
nburn=100
plot(res$llk[nburn:ngibbs],type='l')
#calculate effective sample size
effectiveSize(mcmc(res$betas[nburn:ngibbs,]))
plot(res$betas[nburn:ngibbs,1],type='l')
plot(res$NBN,type='l')
compare1=function(estim,true){
rango=range(c(true,estim))
plot(true,estim,ylim=rango,xlim=rango)
lines(rango,rango,col='red',lwd=2)
}
k=res$betas[ngibbs,]
nparam=ncol(xmat)
k1=matrix(k,nparam,ncomm); round(k1,2)
ordem=rep(NA,ncomm)
for (i in 2:nparam){
ordem[i-1]=which(k1[i,]==max(k1[i,]))
}
round(k1[,ordem],2)
image(k1[,ordem])
#look at nlk
par(mfrow=c(1,1))
tmp=matrix(res$nlk[ngibbs,],nloc,ncomm);
boxplot(tmp)
compare1(estim=jitter(tmp[,ordem]),true=jitter(nlk.true))
#look at betas
k=matrix(res$betas[ngibbs,],nparam,ncomm)
compare1(estim=k[,ordem],true=betas.true)
#look at phi
tmp=matrix(res$phi[ngibbs,],ncomm,nspp)
tmp1=tmp[ordem,]
compare1(estim=tmp1,true=phi.true)
